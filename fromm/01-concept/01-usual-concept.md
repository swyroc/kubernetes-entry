# Concept
* Faas - Function as a service

# 服务注册 服务发现
```
为什么会衍生？
因为从巨石应用衍生到多层应用衍生到微服务，一个哪怕很简单的应用都可以被拆分成多个应用，拆分出来的每个应用都可以被一个微型团队自我迭代，互联网是粗暴的，面对着微服务，很多互联网公司甚至将一个简单的应用拆分成多个应用，原来的一个应用甚至被拆分成30到50个应用。
1. 30~50 - 微型微服务，那么这个对于运维来说简直就是噩梦。而且这些应用形成了复杂的网状模型。一个应用可能与多个其他的应用之间形成复杂的关系。
2. 非静态配置，动态服务发现 - 原有的服务与服务之间的调用都不是通过静态配置的方式来实现，动态服务发现就衍生了服务总线的概念。每个对外提供服务的应用都可以再总线上注册服务，一旦某个应用需要某种类型服务，那么该应用就可以到服务总线上去查找某个服务，通过调用接口的方式来访问提供服务的客户端，一旦某服务出现故障，那么总线就可以将该服务移除。所以总线只要没有问题，那么服务调用就不会有问题。
3. 服务编排 - 30个服务也同时诞生了一个问题就是服务部署，怎样将这些微服务规划并部署，也是一个问题。可以提供一组主机，这些主机上面运行服务编排系统，一旦某个服务需要部署，那么只需要将该服务编排系统提供给服务，那么服务编排系统就会自动的查询当前主机中最适合部署服务的主机，然后将应用服务部署上去。
4. 容器编排系统 - 不可避免的问题就是，肯定有特殊应用对自己的部署环境有一定的要求，比如当某些主机组成的集群中的任何一台主机都无法满足应用部署的需求的时候，那么此时部署就成为一个问题。可以让应用服务自身容器化，自带部署所需要的环境。服务编排系统面临最大的问题就是底层系统环境是否满足部署应用程序运行逻辑，那么容器化技术的出现刚好解决了这个问题。打包运行应用程序的方式-镜像，运行应用程序的方式-容器。简言之，通过docker打包应用程序，跑应用程序不是通过直接部署应用程序到底层，而是将应用程序的镜像下载到对应节点上，启动为容器，镜像自带了应用程序依赖的所有环境，因此可以一次打包到处运行，而无需再考虑底层应用环境异构与否。单一的容器没有价值，只有将容器编排之后，容器的价值才能发挥出来。
    docker解决了应用程序打包的难题，推动了容器技术的快速普及
    容器本身提供了托管应用程序应用的底层逻辑，容器编排Orchestration才是真正产生价值的所在。


```

## 容器编排系统 - Container Orchestration
* 容器编排系统用来管理容器的生命周期，特别是在大型的动态的环境中使用会更好
* 容器编排系统可以完成以下内容：<br>
    a. 容器的提供和部署 - 如何获得容器镜像，如何部署容器，如何运行容器<br>
    b. 容器的冗余和可用性 - 一旦容器发生故障，如何通过机制将容器自动的恢复过来，步骤必须是自动的<br>
    c. 按需分配，扩容伸缩，如何根据某种机制自动的进行扩容和伸缩。节点不够扩容，资源闲置时缩容<br>
    d. 一旦底层资源紧张，那么自动将容器迁移到别的节点，用户对此无感知<br>
    e. 容器之间进行资源分配，甚至在某种情况下将集群内部的资源暴露到集群外部去，以便对外提供访问<br>
    f. 负载均衡，以及容器之间的服务发现<br>
    g. 健康监测-health check<br>
    h. 应用程序的配置等<br>
以上所有功能必须都是自动完成的。总结以下最主要的几个功能：<br>
    a. Service Discovery<br>
    b. Load Balancing<br>
    c. Secrets/configuration/storage management<br>
    d. Health checks<br>
    f. Auto-Scaling/restart/healing of containers and hosts<br>
    g. Zero-downtime deploys<br>

### Docker Swarm 与K8S之间的比较
* Docker已死，含义是docker技术越来越火，可是docker这家公司却活不下去了快
* 为什么docker内置了docker swarm，但是docker官宣却说支持k8s？
* K8s的前生是google十几年前遇到容器编排问题而研发的内部项目Borg，只是当下使用golang语言重新开发了Borg产生了k8s，十几年的容器编排经验导致了google在容器编排领域趟过了无数的坑，积累了无数的经验，这点其实引人深思。
* Docker只是一家互联网公司，而其本身并不知道容器编排是为了解决什么，与google相比，太弱了。


# K8S的习惯
从上面可以看到,k8s管理的底层肯定是一个集群，在集群上运行容器的应用。没有了集群k8s就玩不了了，因为涉及到容器编排。底层可以是真实的物理机主机，也可以是一些虚拟机虚拟化资源。
* 底层主机中，可以有多个Master，多个node，其中master被称为control panel，而node被称为data node。master是管理控制，而Node是真正干活的。
* 多个master是为了冗余备用，一旦其中一个master宕机，另外的master可以快速接管，而多个node是为了负载均衡以及冗余，多个node之间都需要同时工作，每个node做的事情都不相同
* 构建K8S集群，至少需要2个node，一个master。且这只是为了测试使用。生产环境至少要有2个以上的master，而node取决于使用场景。一般而言master3个就够了。
* master的作用更多的是控制，其可以将所有的node组成一个资源池，master清晰了解资源池中的每个节点当下的空闲的cpu以及内存等信息，当有新的容器要被跑在node上的时候，master要知道将容器部署到哪个容器最好。
* k8s运行的最小单元并不是node，而是称作pod的组件。


## K8S 之 master
* 所有的指令，API也好，UI也好，CLI也好，都必须经过master然后才能继续向下发，去某个Node去运行。
* Master由API Server, Scheduler, Controller，etcd组成，前三个是由k8s自己提供，而后面的是一个叫作CoreOS的公司提供，etcd可以想象成就是一个类似于Redis的k-v存储系统。后来被红帽收购

### Scheduler
调度器。用来评估到底哪个Node是最佳目标节点,选取哪个Node作为target node。调度完成之后，Scheduler会把选举的结果也保存在Master的ETCD中。Scheduler并不直接指挥Node去完成具体的工作，而是交由Controller来完成。
<br>
程序名叫kube-scheduler。

### Controller
控制器。K8S提供的API是声明式API。即K8S有自己一套完备的逻辑去实现调用者的需求，而不需要调用者去关心调用的细节。调用者只需要告诉K8S需要执行什么即可。Controller需要去负责具体的执行细节。是K8S Master的大脑核心。Controller会使用到control loop。
<br>
程序名叫kube-controller-manager.

### API Server
整个K8S的功能要提供给客户端来调用，因此API Server是K8S唯一接受请求的入口。用于检查用户请求是否合规，如果合规就将请求保存在ETCD中。用户所有的请求都会到达api server，api server存储用户的请求，controller始终watch着api server中的资源变动需求。同样Scheduler也始终watch着api server。在K8S中，资源始终有两种状态，其中一个是用户请求并保存在ETCD中用户期待的状态，另外一个是真实的实际的运行状态。Controller始终检查对比着这2种状态。如果状态不一致，那么controller需要负责确保状态一致。<br>
程序名叫kube.

### ETCD
ETCD用户保存用户所描述的期待的容器的状态，因为用户的需求大多数是不可控的，为了最大限度下确保用户的请求是合规的，所以API Server对用户能发出的请求做了进一步的约束，规定哪些对于ETCD的请求是合规的，用户所发出的请求必须满足哪些规范。

### Node Kublet
Node上的第一个组件,每个Node上也运行着kublet，kublet也时刻watch着api server上的资源变动，当scheduler完成调度选取之后，也会将调度结果保存在master etcd中，当api server上有资源变动的时候，kublet监测到这个资源变动是交给自己所在的node来负责，那么kublet就会运行执行任务。首先kublet会去找docker，docker会负责去执行下载镜像，创建容器等动作。

### Node docker
Node上面的第二个重要的组件。K8S支持的标准的容器引擎。K8S也支持RKT等其他容器引擎。对于其他引擎是通过CRI插件来支持。只要该引擎能对接到CRI那么K8S就能识别并运行。

### POD
POD才是K8S运行的基本原子单元。容器之外又加了一层壳，这层外壳就是POD，一个POD中可能存在多个容器，这些容器被当作原子单元管理。一个node中可以有多个POD.


#### Linux的底层特性
* PID
* Network
* Mount
* IPC
* USER
* UTS
同一个POD中的多个容器，共享UTS, IPC, Network。













